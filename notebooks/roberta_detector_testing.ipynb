{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration aadityaubhat--GPT-wiki-intro-10ad8b711a5f3880\n",
      "Found cached dataset csv (C:/Users/andre/.cache/huggingface/datasets/aadityaubhat___csv/aadityaubhat--GPT-wiki-intro-10ad8b711a5f3880/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'real', 'fake', 'title_len', 'wiki_intro_len', 'generated_intro_len', 'prompt', 'generated_text', 'prompt_tokens', 'generated_text_tokens'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'real', 'fake', 'title_len', 'wiki_intro_len', 'generated_intro_len', 'prompt', 'generated_text', 'prompt_tokens', 'generated_text_tokens'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "ds = load_dataset(\"aadityaubhat/GPT-wiki-intro\", split=\"train\")\n",
    "ds = ds.rename_column(\"generated_intro\", \"fake\")\n",
    "ds = ds.rename_column(\"wiki_intro\", \"real\")\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.33333)\n",
    "ds = ds.with_format(\"torch\")\n",
    "ds_test = ds['test']\n",
    "ds_train = ds['train']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:10<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.981\n",
      "\n",
      "Testing positives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-openai-detector\", max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-openai-detector\").to(device)\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-classification\", device=0, model=model, tokenizer=tokenizer)\n",
    "\n",
    "labels = list(model.config.id2label.values())\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "dataset = ds['test'].select(range(1000))\n",
    "print(\"Testing negatives...\")\n",
    "for out in tqdm(pipe(KeyDataset(dataset, \"real\"), batch_size=64, truncation=True), total=len(dataset)):\n",
    "    if out['label'] == labels[0]:\n",
    "        false_pos += 1\n",
    "    else:\n",
    "        true_neg += 1\n",
    "print(f\"Acc: {true_neg / (true_neg + false_pos)}\\n\")\n",
    "\n",
    "true_pos = 0\n",
    "false_neg = 0\n",
    "print(\"Testing positives...\")\n",
    "for out in tqdm(pipe(KeyDataset(dataset, \"fake\"), batch_size=64, truncation=True), total=len(dataset)):\n",
    "    if out['label'] == labels[0]:\n",
    "        true_pos += 1\n",
    "    else:\n",
    "        false_neg += 1\n",
    "print(f\"Acc: {true_pos / (true_pos + false_neg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:           0.811\n",
      "Precision:          0.9712121212121212\n",
      "Recall:             0.641\n",
      "n-samples:          2000\n",
      "\n",
      "False positives:    0.0095\n",
      "False negatives:    0.1795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot = true_pos+true_neg+false_pos+false_neg\n",
    "print(f\"\"\"\n",
    "Accuracy:           {(true_pos+true_neg) / (tot)}\n",
    "Precision:          {true_pos/ (true_pos + false_pos)}\n",
    "Recall:             {true_pos/ (true_pos + false_neg)}\n",
    "n-samples:          {tot}\n",
    "\n",
    "False positives:    {false_pos / tot}\n",
    "False negatives:    {false_neg / tot}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7373350858688354}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"I like you. I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3974\n",
      "1465\n"
     ]
    }
   ],
   "source": [
    "def tokenized_length(text):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "longest = max(ds_test['real'], key=tokenized_length, )\n",
    "tokens = tokenizer(longest, return_tensors=\"pt\")['input_ids'][0]\n",
    "print(len(longest))\n",
    "print(len(tokens))\n",
    "\n",
    "truncated_tokens = tokenizer(longest, truncation=True, return_tensors=\"pt\")['input_ids'][0]\n",
    "truncated_text = tokenizer.decode(truncated_tokens)\n",
    "print(len(truncated_text))\n",
    "print(len(longest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = ds.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 0.002\n",
    "variance_limit = 1e-10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
