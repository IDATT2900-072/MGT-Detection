{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Execution file for generating Research Abstract Dataset\n",
    "See 'dataset_generator.py' for implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "from dataset_generator import count_and_reformat, filter_list, generate_abstracts, generate_GPT_abstract, get_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load and preprocess dataset\n",
    "This will take some time if dataset is large. This is only necessary to do once each jupyter-session as local variables are stored until session/kernel shut down."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/nicolaisivesind/.cache/huggingface/datasets/gfissore___json/gfissore--arxiv-abstracts-2021-23556c248bdbe0fc/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc7d7a72a2094e8f98773736561c823e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Counting words: 100%"
     ]
    }
   ],
   "source": [
    "# Code execution\n",
    "dataset = ds.load_dataset(\"gfissore/arxiv-abstracts-2021\")['train']\n",
    "reformatted_dataset = count_and_reformat(dataset=dataset,\n",
    "                                  count_column='abstract',\n",
    "                                  retain_columns=['title', 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run code segments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 143 elements matching the filter.\n",
      "Returned list is of length 100.\n",
      "\n",
      "Found 30589 elements matching the filter.\n",
      "Returned list is of length 1400.\n",
      "\n",
      "Found 139106 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "\n",
      "Found 245943 elements matching the filter.\n",
      "Returned list is of length 2000.\n",
      "\n",
      "Found 428437 elements matching the filter.\n",
      "Returned list is of length 2000.\n",
      "\n",
      "Found 569658 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "\n",
      "Found 463666 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "\n",
      "Found 143 elements matching the filter.\n",
      "Returned list is of length 100.\n",
      "\n",
      "Found 30589 elements matching the filter.\n",
      "Returned list is of length 1400.\n",
      "\n",
      "Found 139106 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "\n",
      "Found 245943 elements matching the filter.\n",
      "Returned list is of length 2000.\n",
      "\n",
      "Found 428437 elements matching the filter.\n",
      "Returned list is of length 2000.\n",
      "\n",
      "Found 569658 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "\n",
      "Found 463666 elements matching the filter.\n",
      "Returned list is of length 1500.\n",
      "{'title': 'Boxicity and Poset Dimension', 'abstract': '  Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge\\nset $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals\\n$[a_1,b_1]\\\\times [a_2,b_2]\\\\times...\\\\times [a_k,b_k]$. The {\\\\it boxicity} of\\n$G$, $\\\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as\\nthe intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to\\na $k$-dimensional box and two vertices are adjacent in $G$ if and only if their\\ncorresponding boxes intersect. Let $\\\\poset=(S,P)$ be a poset where $S$ is the\\nground set and $P$ is a reflexive, anti-symmetric and transitive binary\\nrelation on $S$. The dimension of $\\\\poset$, $\\\\dim(\\\\poset)$ is the minimum\\ninteger $t$ such that $P$ can be expressed as the intersection of $t$ total\\norders. Let $G_\\\\poset$ be the \\\\emph{underlying comparability graph} of\\n$\\\\poset$, i.e. $S$ is the vertex set and two vertices are adjacent if and only\\nif they are comparable in $\\\\poset$. It is a well-known fact that posets with\\nthe same underlying comparability graph have the same dimension. The first\\nresult of this paper links the dimension of a poset to the boxicity of its\\nunderlying comparability graph. In particular, we show that for any poset\\n$\\\\poset$, $\\\\boxi(G_\\\\poset)/(\\\\chi(G_\\\\poset)-1) \\\\le \\\\dim(\\\\poset)\\\\le\\n2\\\\boxi(G_\\\\poset)$, where $\\\\chi(G_\\\\poset)$ is the chromatic number of $G_\\\\poset$\\nand $\\\\chi(G_\\\\poset)\\\\ne1$. It immediately follows that if $\\\\poset$ is a height-2\\nposet, then $\\\\boxi(G_\\\\poset)\\\\le \\\\dim(\\\\poset)\\\\le 2\\\\boxi(G_\\\\poset)$ since the\\nunderlying comparability graph of a height-2 poset is a bipartite graph. The\\nsecond result of the paper relates the boxicity of a graph $G$ with a natural\\npartial order associated with the \\\\emph{extended double cover} of $G$, denoted\\nas $G_c$: Note that $G_c$ is a bipartite graph with partite sets $A$ and $B$\\nwhich are copies of $V(G)$ such that corresponding to every $u\\\\in V(G)$, there\\nare two vertices $u_A\\\\in A$ and $u_B\\\\in B$ and $\\\\{u_A,v_B\\\\}$ is an edge in\\n$G_c$ if and only if either $u=v$ or $u$ is adjacent to $v$ in $G$. Let\\n$\\\\poset_c$ be the natural height-2 poset associated with $G_c$ by making $A$\\nthe set of minimal elements and $B$ the set of maximal elements. We show that\\n$\\\\frac{\\\\boxi(G)}{2} \\\\le \\\\dim(\\\\poset_c) \\\\le 2\\\\boxi(G)+4$. These results have\\nsome immediate and significant consequences. The upper bound $\\\\dim(\\\\poset)\\\\le\\n2\\\\boxi(G_\\\\poset)$ allows us to derive hitherto unknown upper bounds for poset\\ndimension such as $\\\\dim(\\\\poset)\\\\le 2\\\\tw(G_\\\\poset)+4$, since boxicity of any\\ngraph is known to be at most its $\\\\tw+2$. In the other direction, using the\\nalready known bounds for partial order dimension we get the following: (1) The\\nboxicity of any graph with maximum degree $\\\\Delta$ is $O(\\\\Delta\\\\log^2\\\\Delta)$\\nwhich is an improvement over the best known upper bound of $\\\\Delta^2+2$. (2)\\nThere exist graphs with boxicity $\\\\Omega(\\\\Delta\\\\log\\\\Delta)$. This disproves a\\nconjecture that the boxicity of a graph is $O(\\\\Delta)$. (3) There exists no\\npolynomial-time algorithm to approximate the boxicity of a bipartite graph on\\n$n$ vertices with a factor of $O(n^{0.5-\\\\epsilon})$ for any $\\\\epsilon>0$,\\nunless $NP=ZPP$.\\n', 'word_count': 577}\n"
     ]
    }
   ],
   "source": [
    "dataset_400_600 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=400,\n",
    "                              word_count_max=600,\n",
    "                              quantity=100)\n",
    "dataset_300_399 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=300,\n",
    "                              word_count_max=399,\n",
    "                              quantity=1400)\n",
    "dataset_250_299 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=250,\n",
    "                              word_count_max=299,\n",
    "                              quantity=1500)\n",
    "dataset_200_249 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=200,\n",
    "                              word_count_max=249,\n",
    "                              quantity=2000)\n",
    "dataset_150_199 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=150,\n",
    "                              word_count_max=199,\n",
    "                              quantity=2000)\n",
    "dataset_100_149 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=100,\n",
    "                              word_count_max=149,\n",
    "                              quantity=1500)\n",
    "dataset_50_99 = filter_list(data=reformatted_dataset,\n",
    "                              word_count_min=50,\n",
    "                              word_count_max=99,\n",
    "                              quantity=1500)\n",
    "print(dataset_400_600[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-file already exists. Will append new rows to existing document. Cancel execution if this is not intended.\n",
      "\n",
      " Generating: 100/39Abstract generation complete.\n",
      "CSV-file already exists. Will append new rows to existing document. Cancel execution if this is not intended.\n",
      "\n",
      " Generating: 264/1400"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "API-error: 429, {\n  \"error\": {\n    \"message\": \"That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 11f2958d24004ff216ab452cc50f34bb in your message.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m generate_abstracts(data\u001B[38;5;241m=\u001B[39mdataset_400_600,\n\u001B[1;32m      2\u001B[0m                    target_file_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresearch_abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      3\u001B[0m                    target_dir_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./../../datasets/origins/research-abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      4\u001B[0m                    start_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m61\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mgenerate_abstracts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_300_399\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mtarget_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresearch_abstracts\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mtarget_dir_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./../../datasets/origins/research-abstracts\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m generate_abstracts(data\u001B[38;5;241m=\u001B[39mdataset_250_299,\n\u001B[1;32m      9\u001B[0m                    target_file_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresearch_abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     10\u001B[0m                    target_dir_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./../../datasets/origins/research-abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m generate_abstracts(data\u001B[38;5;241m=\u001B[39mdataset_200_249,\n\u001B[1;32m     12\u001B[0m                    target_file_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresearch_abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     13\u001B[0m                    target_dir_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./../../datasets/origins/research-abstracts\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/NTNU/Bachelor/Kode/MGT-Detection/helpers/dataset_generator.py:169\u001B[0m, in \u001B[0;36mgenerate_abstracts\u001B[0;34m(data, target_file_name, target_dir_path, start_index, iterate_forward, debug)\u001B[0m\n\u001B[1;32m    166\u001B[0m if debug:\n\u001B[1;32m    167\u001B[0m     continue\n\u001B[0;32m--> 169\u001B[0m # Generate abstract\n\u001B[1;32m    170\u001B[0m generated_abstract, generated_word_count = generate_GPT_abstract(system_prompt, user_prompt)\n\u001B[1;32m    172\u001B[0m # Write to CSV\n",
      "File \u001B[0;32m~/Desktop/NTNU/Bachelor/Kode/MGT-Detection/helpers/dataset_generator.py:202\u001B[0m, in \u001B[0;36mgenerate_GPT_abstract\u001B[0;34m(system_prompt, user_prompt)\u001B[0m\n\u001B[1;32m    199\u001B[0m         generated_word_count = length_of(generated_abstract)\n\u001B[1;32m    200\u001B[0m         return generated_abstract, generated_word_count\n\u001B[1;32m    201\u001B[0m     else:\n\u001B[0;32m--> 202\u001B[0m         # Quit if something fails to not waste API-usage\n\u001B[1;32m    203\u001B[0m         raise RuntimeError(f\"API-error: {response.status_code}, {response.text}\")\n\u001B[1;32m    206\u001B[0m def get_models():\n\u001B[1;32m    207\u001B[0m     # Set up headers for the request\n",
      "\u001B[0;31mRuntimeError\u001B[0m: API-error: 429, {\n  \"error\": {\n    \"message\": \"That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 11f2958d24004ff216ab452cc50f34bb in your message.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "#generate_abstracts(data=dataset_400_600,\n",
    "#                   target_file_name='research_abstracts',\n",
    "#                   target_dir_path='./../../datasets/origins/research-abstracts',\n",
    "#                   start_index=264)\n",
    "generate_abstracts(data=dataset_300_399,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts',\n",
    "                   start_index=264)\n",
    "generate_abstracts(data=dataset_250_299,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')\n",
    "generate_abstracts(data=dataset_200_249,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')\n",
    "generate_abstracts(data=dataset_150_199,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')\n",
    "generate_abstracts(data=dataset_100_149,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')\n",
    "generate_abstracts(data=dataset_50_99,\n",
    "                   target_file_name='research_abstracts',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}