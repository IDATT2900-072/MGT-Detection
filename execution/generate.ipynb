{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Execution file for generating Research Abstract Dataset\n",
    "See 'dataset_generator.py' for implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import datasets as ds\n",
    "\n",
    "from data_manipulation.data_processing import count_and_reformat, filter_list, sample_uniform_subset, substitute_duplicates_uniform\n",
    "from data_manipulation.data_generation import generate_abstracts, generate_GPT_abstract, get_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load and preprocess dataset\n",
    "This will take some time if dataset is large. This is only necessary to do once each jupyter-session as local variables are stored until session/kernel shut down."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/nicolaisivesind/.cache/huggingface/datasets/gfissore___json/gfissore--arxiv-abstracts-2021-23556c248bdbe0fc/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "936c1ec783674666be3a11f8dd7900e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Counting words: 1%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Code execution\u001B[39;00m\n\u001B[1;32m      2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mload_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgfissore/arxiv-abstracts-2021\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 3\u001B[0m reformatted_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mcount_and_reformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mcount_column\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mabstract\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mretain_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mabstract\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/NTNU/Bachelor/Kode/MGT-Detection/execution/../data_manipulation/data_processing.py:102\u001B[0m, in \u001B[0;36mcount_and_reformat\u001B[0;34m(dataset, count_column, retain_columns)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# Format data_points and retrieve word_count\u001B[39;00m\n\u001B[1;32m    101\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset)\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data_point \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mint\u001B[39m(total \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCounting words: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(i\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mtotal\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2293\u001B[0m, in \u001B[0;36mDataset.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2291\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(pa_subtable\u001B[38;5;241m.\u001B[39mnum_rows):\n\u001B[1;32m   2292\u001B[0m             pa_subtable_ex \u001B[38;5;241m=\u001B[39m pa_subtable\u001B[38;5;241m.\u001B[39mslice(i, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m-> 2293\u001B[0m             formatted_output \u001B[38;5;241m=\u001B[39m \u001B[43mformat_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2294\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpa_subtable_ex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2295\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2296\u001B[0m \u001B[43m                \u001B[49m\u001B[43mformatter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2297\u001B[0m \u001B[43m                \u001B[49m\u001B[43mformat_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_format_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2298\u001B[0m \u001B[43m                \u001B[49m\u001B[43moutput_all_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_output_all_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2299\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2300\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m formatted_output\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py:634\u001B[0m, in \u001B[0;36mformat_table\u001B[0;34m(table, key, formatter, format_columns, output_all_columns)\u001B[0m\n\u001B[1;32m    632\u001B[0m python_formatter \u001B[38;5;241m=\u001B[39m PythonFormatter(features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m format_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 634\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    636\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m format_columns:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py:406\u001B[0m, in \u001B[0;36mFormatter.__call__\u001B[0;34m(self, pa_table, query_type)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable, query_type: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 406\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_row\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m query_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    408\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat_column(pa_table)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py:442\u001B[0m, in \u001B[0;36mPythonFormatter.format_row\u001B[0;34m(self, pa_table)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LazyRow(pa_table, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    441\u001B[0m row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_arrow_extractor()\u001B[38;5;241m.\u001B[39mextract_row(pa_table)\n\u001B[0;32m--> 442\u001B[0m row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpython_features_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_row\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m row\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/formatting/formatting.py:225\u001B[0m, in \u001B[0;36mPythonFeaturesDecoder.decode_row\u001B[0;34m(self, row)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_row\u001B[39m(\u001B[38;5;28mself\u001B[39m, row: \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[0;32m--> 225\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_example\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01melse\u001B[39;00m row\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/datasets/features/features.py:1869\u001B[0m, in \u001B[0;36mFeatures.decode_example\u001B[0;34m(self, example, token_per_repo_id)\u001B[0m\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode_example\u001B[39m(\u001B[38;5;28mself\u001B[39m, example: \u001B[38;5;28mdict\u001B[39m, token_per_repo_id: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1851\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001B[39;00m\n\u001B[1;32m   1852\u001B[0m \n\u001B[1;32m   1853\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1861\u001B[0m \u001B[38;5;124;03m        `dict[str, Any]`\u001B[39;00m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1864\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m   1865\u001B[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001B[38;5;241m=\u001B[39mtoken_per_repo_id)\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_column_requires_decoding[column_name]\n\u001B[1;32m   1867\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m value\n\u001B[1;32m   1868\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m column_name, (feature, value) \u001B[38;5;129;01min\u001B[39;00m zip_dict(\n\u001B[0;32m-> 1869\u001B[0m             {key: value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m example}, example\n\u001B[1;32m   1870\u001B[0m         )\n\u001B[1;32m   1871\u001B[0m     }\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Code execution\n",
    "dataset = ds.load_dataset(\"gfissore/arxiv-abstracts-2021\")['train']\n",
    "reformatted_dataset = count_and_reformat(dataset=dataset,\n",
    "                                  count_column='abstract',\n",
    "                                  retain_columns=['title', 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run code segments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sorting into lists: 99%\n",
      " Sampling data points: 100%"
     ]
    }
   ],
   "source": [
    "uniform_subset = sample_uniform_subset(dataset=reformatted_dataset,\n",
    "                                       column='word_count',\n",
    "                                       start=50,\n",
    "                                       end=600,\n",
    "                                       subset_size=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-file already exists. Will append new rows to existing document. Cancel execution if this is not intended.\n",
      "\n",
      " Generating: 8773/10000\n",
      " API-error. Reattempting API-call\n",
      " Generating: 10000/10000\n",
      "Abstract generation complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_abstracts(data=uniform_subset,\n",
    "                   target_file_name='research_abstracts-uniform-clean',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts',\n",
    "                   start_index=7315)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/nicolaisivesind/.cache/huggingface/datasets/csv/default-b1e87654a6a27fb5/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dc939b28b0f48f69e74648b9b4bd6f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sorting into lists: 99%\n",
      " Sampling substitutes: 100%list:  10000\n",
      "unique:  10000\n",
      "CSV-file already exists. Will append new rows to existing document. Cancel execution if this is not intended.\n",
      "\n",
      " Generating: 27/27\n",
      "Abstract generation complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = ds.load_dataset('csv', data_files='../../datasets/origins/research-abstracts/research_abstracts-uniform.csv')['train']\n",
    "\n",
    "substitutes = substitute_duplicates_uniform(dataset_2, reformatted_dataset, 'title', 'word_count', 10000, 50, 600, 42)\n",
    "titles = dataset_2.unique('title') + [substitute['title'].replace('\\n', '') for substitute in substitutes]\n",
    "\n",
    "\n",
    "print('list: ', len(titles))\n",
    "print('unique: ', len(set(titles)))\n",
    "\n",
    "generate_abstracts(data=substitutes,\n",
    "                   target_file_name='research_abstracts-uniform',\n",
    "                   target_dir_path='./../../datasets/origins/research-abstracts')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}