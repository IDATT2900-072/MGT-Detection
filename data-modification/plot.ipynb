{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting\n",
    "This book is used for producing various plots related to data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "from data_processing import count_and_reformat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/nicolaisivesind/.cache/huggingface/datasets/gfissore___json/gfissore--arxiv-abstracts-2021-23556c248bdbe0fc/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf62cad3cf1b4639a5a724c62083cfab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Counting words: 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/nicolaisivesind/.cache/huggingface/datasets/csv/default-56ca7a3413583396/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b9d5e33c4034829a952e4abf05b84e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arxiv = count_and_reformat(dataset=load_dataset(\"gfissore/arxiv-abstracts-2021\")['train'],\n",
    "                           count_column='abstract',\n",
    "                           retain_columns=['title', 'abstract'])\n",
    "\n",
    "chatgpt_abstracts = load_dataset('csv', data_files='../../datasets/origins/research-abstracts/research_abstracts-deduplicated.csv')[\n",
    "    'train']\n",
    "\n",
    "\n",
    "#substitutes = substitute_duplicates_uniform(abstracts, arxiv, 'title', 'word_count', 10000, 50, 600, 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plotting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/nicolaisivesind/.cache/huggingface/datasets/csv/default-626230864bac12a1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5438da33e3b946e6a5319c67e48df784"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17bb250c09074e8699d39c0044b0eb98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4fc56ba25b64120a5c22e4d2e281697"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/nicolaisivesind/.cache/huggingface/datasets/csv/default-626230864bac12a1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbeb594f94e94e0f8ed95bfa7f7ac4b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/nicolaisivesind/.cache/huggingface/datasets/csv/default-626230864bac12a1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2068f76a25d42efbeba27cceea62f90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatgpt_abstracts = load_dataset('csv', data_files='../../datasets/origins/research-abstracts/research_abstracts-uniform-clean.csv')[\n",
    "    'train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "#from data_analysis import plot_distribution, plot_histogram\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from data_processing import sample_uniform_subset, completion_bar\n",
    "\n",
    "matplotlib.use('MacOSX')\n",
    "\n",
    "\n",
    "def plot_distribution(plots: list[dict], start, end, sigma=2, x_label=None, y_label=None, save_to=None, title=None, y_lim=None, h_lines=None, v_lines=None, legend_offset=1.0):\n",
    "    # Set the plot style\n",
    "    with plt.style.context('ggplot'):\n",
    "        # Create the figure and axis objects\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.patch.set_facecolor('lightgrey')\n",
    "        ax.patch.set_alpha(0.3)\n",
    "\n",
    "        # Set custom y-axis limits if provided\n",
    "        if y_lim:\n",
    "            ax.set_ylim(y_lim)\n",
    "\n",
    "        # Set horizontal lines if provided\n",
    "        if h_lines:\n",
    "            for h_line in h_lines:\n",
    "                ax.axhline(h_line['value'], color=h_line['color'], linestyle='--', alpha=h_line['alpha'])\n",
    "                ax.text(h_line['offset'][0], h_line['value'] + h_line['offset'][1], h_line['text'], color=h_line['color'])\n",
    "\n",
    "        # Set vertical lines if provided\n",
    "        if v_lines:\n",
    "            for v_line in v_lines:\n",
    "                ax.axvline(v_line['value'], color=v_line['color'], linestyle='--', alpha=0.8)\n",
    "                ax.text(v_line['value'] + v_line['offset'][0], v_line['offset'][1], v_line['text'], color=v_line['color'])\n",
    "\n",
    "        for i, plot in enumerate(plots):\n",
    "            counts = np.zeros(end - start + 1)\n",
    "\n",
    "            for data_point in plot['dataset']:\n",
    "                count = data_point[plot['column_name']]\n",
    "\n",
    "                if start <= count <= end:\n",
    "                    index = data_point[plot['column_name']] - start\n",
    "                    counts[index] += 1\n",
    "\n",
    "            # Apply the Gaussian filter\n",
    "            smoothed_counts = gaussian_filter1d(counts, sigma)\n",
    "\n",
    "            # Plot the smoothed data with a label for the legend\n",
    "            x_values = np.arange(start, end + 1)\n",
    "            ax.plot(x_values, smoothed_counts, label=plot['display'], alpha=plot['alpha'], color=plot['color'])\n",
    "\n",
    "            if plot['mode']:\n",
    "                # Find the maximum y-value and its index in the smoothed_counts array\n",
    "                max_y_index = np.argmax(smoothed_counts)\n",
    "                max_y_value = smoothed_counts[max_y_index]\n",
    "                max_x_value = x_values[max_y_index]\n",
    "\n",
    "                # Draw a dashed line from the maximum y-value to the x-axis\n",
    "                ax.axvline(max_x_value, ymin=0, ymax=max_y_value / ax.get_ylim()[1], color=plot['color'], linestyle='--', alpha=plot['alpha'])\n",
    "\n",
    "                # Display the x-value at the base of the dashed line\n",
    "                ax.text(max_x_value + 2 + 3 * len(str(max_x_value)), 0, f\"{max_x_value}\", color=plot['color'], ha='center', va='bottom')\n",
    "\n",
    "        # Set labels and title if provided\n",
    "        if x_label:\n",
    "            ax.set_xlabel(x_label)\n",
    "        if y_label:\n",
    "            ax.set_ylabel(y_label)\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a legend with an offset\n",
    "        ax.legend(facecolor='white', bbox_to_anchor=(legend_offset, 1))\n",
    "\n",
    "    # Display the plot\n",
    "    plt.subplots_adjust(left=0.07, bottom=0.143, right=0.93, top=0.943)\n",
    "\n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram(plots: list[dict], start, end, sigma=2, save_to=None):\n",
    "    bins = end-start+1\n",
    "\n",
    "    # Set the plot style\n",
    "    with plt.style.context('ggplot'):\n",
    "        # Create the figure and axis objects\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.patch.set_facecolor('lightgrey')\n",
    "        ax.patch.set_alpha(0.3)\n",
    "\n",
    "        for i, plot in enumerate(plots):\n",
    "            counts = []\n",
    "\n",
    "            for data_point in plot['dataset']:\n",
    "                count = data_point[plot['column_name']]\n",
    "\n",
    "                if start <= count <= end:\n",
    "                    counts.append(count)\n",
    "\n",
    "            ax.hist(counts, bins=bins, range=(start, end), alpha=plot['alpha'], label=plot['display'], color=plot['color'])\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Length of text in words')\n",
    "        ax.set_ylabel('Number of texts')\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a legend\n",
    "        ax.legend(facecolor='white')\n",
    "\n",
    "    # Adjust the plot margins\n",
    "    plt.subplots_adjust(left=0.07, bottom=0.143, right=0.93, top=0.943)\n",
    "\n",
    "    # Save and display the plot\n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_mean_y_per_x(x_values, y_values):\n",
    "    xy_dict = {}\n",
    "    for x, y in zip(x_values, y_values):\n",
    "        if x not in xy_dict:\n",
    "            xy_dict[x] = {'sum': 0, 'count': 0}\n",
    "        xy_dict[x]['sum'] += y\n",
    "        xy_dict[x]['count'] += 1\n",
    "\n",
    "    mean_y_values = {x: y_data['sum'] / y_data['count'] for x, y_data in xy_dict.items()}\n",
    "    return list(mean_y_values.keys()), list(mean_y_values.values())\n",
    "\n",
    "def plot_scatter(plots: list[dict], d_lines=None, h_lines=None, v_lines=None, x_label=None, y_label=None, y_lim=None, legend_offset=(1.0, 1.0), average_curve=None, sigma=2, correlations=None):\n",
    "    with plt.style.context('ggplot'):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.patch.set_facecolor('lightgrey')\n",
    "        ax.patch.set_alpha(0.3)\n",
    "\n",
    "        if y_lim:\n",
    "            ax.set_ylim(y_lim)\n",
    "\n",
    "        if h_lines:\n",
    "            for h_line in h_lines:\n",
    "                ax.axhline(h_line['value'], color=h_line['color'], linestyle='--', alpha=h_line['alpha'])\n",
    "                ax.text(h_line['offset'][0], h_line['value'] + h_line['offset'][1], h_line['text'], color=h_line['color'])\n",
    "\n",
    "        if v_lines:\n",
    "            for v_line in v_lines:\n",
    "                ax.axvline(v_line['value'], color=v_line['color'], linestyle='--', alpha=0.8)\n",
    "                ax.text(v_line['value'] + v_line['offset'][0], v_line['offset'][1], v_line['text'], color=v_line['color'])\n",
    "\n",
    "        for plot in plots:\n",
    "            x_values = [data_point[plot['x']] for data_point in plot['dataset']]\n",
    "            y_values = [data_point[plot['y']] for data_point in plot['dataset']]\n",
    "\n",
    "            ax.scatter(x_values, y_values, label=plot['display'], alpha=plot['alpha'], color=plot['color'])\n",
    "\n",
    "            if average_curve:\n",
    "                x_unique, y_mean = calculate_mean_y_per_x(x_values, y_values)\n",
    "                x_unique, y_mean = zip(*sorted(zip(x_unique, y_mean)))\n",
    "                y_mean_filtered = gaussian_filter1d(y_mean, sigma)\n",
    "                ax.plot(x_unique, y_mean_filtered, label=average_curve['display'], color=average_curve['color'], alpha=average_curve['alpha'])\n",
    "\n",
    "            if correlations:\n",
    "                for correlation in correlations:\n",
    "                    interval = correlation.get('interval', (min(x_values), max(x_values)))\n",
    "                    x_interval_values = [x for x in x_values if interval[0] <= x <= interval[1]]\n",
    "                    y_interval_values = [y for x, y in zip(x_values, y_values) if interval[0] <= x <= interval[1]]\n",
    "                    print(len(x_interval_values))\n",
    "\n",
    "                    corr = np.corrcoef(x_interval_values, y_interval_values)[0, 1]\n",
    "                    ax.text(correlation['positioning'][0], correlation['positioning'][1], f\"{correlation['text']} {corr:.2f}\", color=correlation['color'], alpha=correlation['alpha'])\n",
    "\n",
    "        if d_lines:\n",
    "            for d_line in d_lines:\n",
    "                x_start, y_start = d_line['start']\n",
    "                x_increment, y_increment = d_line['increment']\n",
    "                x_max = max(ax.get_xlim())\n",
    "                y_max = max(ax.get_ylim())\n",
    "\n",
    "                x_end = min(x_max, (y_max - y_start) / y_increment * x_increment + x_start)\n",
    "                y_end = x_end * y_increment / x_increment + y_start - x_start * y_increment / x_increment\n",
    "\n",
    "                ax.plot([x_start, x_end], [y_start, y_end], label=d_line['display'], color=d_line['color'], linestyle='--', alpha=d_line['alpha'])\n",
    "\n",
    "\n",
    "        if x_label:\n",
    "            ax.set_xlabel(x_label)\n",
    "        if y_label:\n",
    "            ax.set_ylabel(y_label)\n",
    "\n",
    "    ax.legend(facecolor='white', bbox_to_anchor=(legend_offset[0], legend_offset[1]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_curves(plots, x_label=None, y_label=None, legend_offset=(1.0, 1.0), sigma=2):\n",
    "    for plot in plots:\n",
    "        dataset = plot['dataset']\n",
    "        positive_loss = []\n",
    "        negative_loss = []\n",
    "\n",
    "        for data_point in dataset:\n",
    "            real_word_count = data_point[plot['benchmark']]\n",
    "            generated_word_count = data_point[plot['predicted']]\n",
    "            loss = real_word_count - generated_word_count\n",
    "\n",
    "            if loss > 0:\n",
    "                positive_loss.append((real_word_count, abs(loss)))\n",
    "            else:\n",
    "                negative_loss.append((real_word_count, abs(loss)))\n",
    "\n",
    "        with plt.style.context('ggplot'):\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            ax.patch.set_facecolor('lightgrey')\n",
    "            ax.patch.set_alpha(0.3)\n",
    "\n",
    "            if positive_loss:\n",
    "                x_values, y_values = zip(*positive_loss)\n",
    "                x_unique, y_mean = calculate_mean_y_per_x(x_values, y_values)\n",
    "                x_unique, y_mean = zip(*sorted(zip(x_unique, y_mean)))\n",
    "                y_mean_filtered = gaussian_filter1d(y_mean, sigma)\n",
    "                ax.plot(x_unique, y_mean_filtered, label=plot['positive-display'], color=plot['positive-color'], alpha=plot['alpha'])\n",
    "\n",
    "            if negative_loss:\n",
    "                x_values, y_values = zip(*negative_loss)\n",
    "                x_unique, y_mean = calculate_mean_y_per_x(x_values, y_values)\n",
    "                x_unique, y_mean = zip(*sorted(zip(x_unique, y_mean)))\n",
    "                y_mean_filtered = gaussian_filter1d(y_mean, sigma)\n",
    "                ax.plot(x_unique, y_mean_filtered, label=plot['negative-display'], color=plot['negative-color'], alpha=plot['alpha'])\n",
    "\n",
    "            if x_label:\n",
    "                ax.set_xlabel(x_label)\n",
    "            if y_label:\n",
    "                ax.set_ylabel(y_label)\n",
    "\n",
    "            ax.legend(facecolor='white', bbox_to_anchor=(legend_offset[0], legend_offset[1]))\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sorting into lists: 99%\n",
      " Sampling data points: 100%"
     ]
    }
   ],
   "source": [
    "arxiv_10k = random.sample(arxiv, k=10000)\n",
    "uniform = sample_uniform_subset(arxiv, 'word_count', 10000, 50, 600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_distribution(plots=[{'dataset': arxiv, 'column_name': 'word_count', 'color': 'red', 'alpha': 0.2,\n",
    "                       'display': 'Source dataset (2 000 000 data points)'},\n",
    "                         {'dataset': chatgpt_abstracts, 'column_name': 'real_word_count', 'color': 'blue', 'alpha': 0.6,\n",
    "                       'display': 'Selected real abstracts (10 000 data points, uniform word count selection)'},\n",
    "                      {'dataset': chatgpt_abstracts, 'column_name': 'generated_word_count', 'color': 'limegreen', 'alpha': 0.8,\n",
    "                       'display': 'Generated abstracts (10 000 data points\\n'\n",
    "                                  '                                    Instructed to match real abstract word count)'}],\n",
    "                     start=50,\n",
    "                     end=600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "plot_distribution(plots=[{'dataset': arxiv, 'column_name': 'word_count', 'color': 'darkmagenta', 'alpha': 0.6,\n",
    "                          'display': 'arXiv-abstracts-2021 (~2m)', 'mode':False},\n",
    "                         {'dataset': arxiv_10k, 'column_name': 'word_count', 'color': 'red', 'alpha': 0.6,\n",
    "                          'display': 'arXiv-abstracts-2021 random subset (10k)', 'mode':True},\n",
    "                         {'dataset': chatgpt_abstracts, 'column_name': 'real_word_count', 'color': 'blue', 'alpha': 0.6,\n",
    "                          'display': 'ChatGPT-Research-Abstracts, real (10k)', 'mode':False},\n",
    "                         {'dataset': chatgpt_abstracts, 'column_name': 'generated_word_count', 'color': 'limegreen', 'alpha': 0.8,\n",
    "                          'display': 'ChatGPT-Research-Abstracts, generated (10k)', 'mode':True}],\n",
    "                   h_lines=[{'value': 18, 'color': 'grey', 'alpha': 0.8, 'text': 'IASS_10k ≈ 18', 'offset': (400, 1)}],\n",
    "                   v_lines=[{'value': 364, 'color': 'grey', 'alpha': 0.8, 'text': 'WC = 361', 'offset': (5, 50)}],\n",
    "                   start=50,\n",
    "                   end=600,\n",
    "                   x_label='WC (length of data points in words)',\n",
    "                   y_label='n (number of data points)',\n",
    "                   y_lim=(0, 75),\n",
    "                   legend_offset=1.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "plot_histogram(plots=[{'dataset': arxiv, 'column_name': 'word_count', 'color': 'red', 'alpha': 0.6,\n",
    "                       'display': 'Source dataset (2 000 000 data points)'},\n",
    "                      {'dataset': chatgpt_abstracts, 'column_name': 'real_word_count', 'color': 'blue', 'alpha': 0.4,\n",
    "                       'display': 'Selected real abstracts (10 000 data points)'},\n",
    "                      {'dataset': chatgpt_abstracts, 'column_name': 'generated_word_count', 'color': 'limegreen', 'alpha': 0.8,\n",
    "                       'display': 'Generated abstracts (10 000 data points\\n'\n",
    "                                  'Instructed to match real abstract word counts)'}],\n",
    "                     start=350,\n",
    "                     end=600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9858\n",
      "143\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "plot_scatter(plots=[{'dataset': chatgpt_abstracts, 'x': 'real_word_count', 'y': 'generated_word_count', 'color': 'blue', 'alpha': 0.1,\n",
    "                     'display': 'Data point in ChatGPT-Research-Abstracts'}],\n",
    "             correlations=[{'interval': (50, 400), 'text': 'r of x ∈ [50, 400]   =', 'positioning':(460, 60), 'color':'magenta', 'alpha':0.8},\n",
    "                           {'interval': (400, 600), 'text': 'r of x ∈ [400, 600] =', 'positioning':(460, 40), 'color':'green', 'alpha':0.8},\n",
    "                           {'interval': (50, 600), 'text': 'r of x ∈ [50, 600]   =', 'positioning':(460, 20), 'color':'darkblue', 'alpha':0.8}],\n",
    "             d_lines=[{'start': (0, 0), 'increment': (1, 1), 'color': 'orange', 'alpha': 0.8, 'display': 'Perfect correlation', 'offset': (0, 0)}],\n",
    "             x_label='x: Real abstract word count',\n",
    "             y_label='y: Generated abstract word count',\n",
    "             y_lim=(0, 600),\n",
    "             legend_offset=(0.5, 0.95),\n",
    "             average_curve={'color': 'red', 'alpha': 0.8, 'display': 'Average word count correlation', 'offset': (10, 10)},\n",
    "             sigma=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_loss_curves(plots=[{'dataset': chatgpt_abstracts, 'benchmark': 'real_word_count', 'predicted': 'generated_word_count', 'positive-color': 'blue','negative-color': 'red', 'alpha': 0.6, 'positive-display': 'Positive loss', 'negative-display': 'Negative loss'}],\n",
    "                x_label='Absolute loss',\n",
    "                y_label='Word count goal',\n",
    "                legend_offset=(0.35, 0.95),\n",
    "                sigma=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}